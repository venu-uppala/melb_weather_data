# ğŸŒ¤ Melbourne Weather data Pipeline

A modular Python pipeline that fetches weather data from an external API, stores it in a PostgreSQL database, and ensures robust logging, schema creation, and batch inserts.

---

## ğŸš€ Features

- âœ… Fetches paginated weather data from external APIs
- âœ… Filters weather data where location is null to provide quality data to analysts
- âœ… Creates `weather_data` table on the fly (idempotent)
- âœ… Inserts records using efficient batch inserts (`executemany`)
- âœ… Skips duplicates with `ON CONFLICT DO NOTHING`
- âœ… Modular logging and configuration
- âœ… Pytest coverage for table creation and data insertion

---

## ğŸ§± Project Structure
<pre>melb_weather_data
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ weather_api.py
â”‚   â”œâ”€â”€ weather_config.py
â”‚   â”œâ”€â”€ weather_database.py
â”‚   â””â”€â”€ weather_logger.py
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_pipeline.py
    â”œâ”€â”€ test_weather_api.py
    â””â”€â”€ test_weather_database.py
</pre>
---

## âš™ï¸ Setup

### 1. Clone the repo

```bash
git clone https://github.com/venu-uppala/melb_weather_data.git
cd melb_weather_data
```

### 2. Setup virtual environment and Install dependencies
```bash
make setup
make activate
```

### 3. Configure database
Edit src/weather_config.py
```Python
DB_CONFIG = {
        "dbname": "your_db",
        "user": "your_user",
        "password": "your_password",
        "host": "localhost",
        "port": "5432"
}
```
ğŸ›  Usage
### Run the weather data pipeline
```Bash
python3 -m src.pipeline
```
This will fail if postgress db is not setup and configure properly, let us run this code using docker. Here docker-compose is used to setup postgres db on the fly, creates weather_data table and loads 1000 records into the table.
```Bash
docker-compose up --build
```
Go to `exec` tab of postgres docker container to execute below listed commands to see the ingested data or use GUI tool like Dbeaver to connect to the postgres db.
```Bash
# To connect to the postgres db created on the fly using docker-compose
psql -U postgres -d melb_weather_data
# count query
select count(1) from weather_data;
# Analyst query
SELECT sensorlocation, AVG(airtemperature) AS avg_temp, AVG(relativehumidity) AS avg_humidity
FROM weather_data
GROUP BY sensorlocation
ORDER BY avg_temp DESC;
```
Below screenshot shows the out of the above mentioned commands.

<img width="926" height="469" alt="image" src="https://github.com/user-attachments/assets/8655bbc0-256f-4844-b420-cc798b654b3f" />

ğŸ§ª Testing
Run all tests with:
```Bash
make test
```
Includes:
- âœ… test_fetch_exact_record_limit
- âœ… test_insert_weather_data_executes_batch_insert
- âœ… test_create_weather_table_executes_sql_and_commits


ğŸ“‹ Table Schema
```Sql
CREATE TABLE IF NOT EXISTS weather_data (
    device_id TEXT NOT NULL,
    sensorlocation TEXT NOT NULL,
    received_at TIMESTAMP NOT NULL,
    airtemperature REAL,
    relativehumidity REAL,
    PRIMARY KEY (device_id, received_at)
);
```
### Query for Analysts
```Sql
SELECT sensorlocation, AVG(airtemperature) AS avg_temp, AVG(relativehumidity) AS avg_humidity
FROM weather_data
GROUP BY sensorlocation
ORDER BY avg_temp DESC;
```

ğŸ“š Logging:
Logs are configured via weather_logger.py and include:
- logger.info() for successful operations
- logger.error() for insert failures
- logger.critical() for connection or transaction issues

ğŸ§  Author
Built by Venu Gopal Uppala â€” Big Data Lead Engineer passionate about scalable data platforms, automation, and team enablement.

ğŸ“„ License
MIT License â€” feel free to use, modify, and contribute.
